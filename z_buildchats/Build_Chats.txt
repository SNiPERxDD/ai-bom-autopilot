[11/08/25, 2:30:24 PM] ∆: Master — read your patch list and baked every fix into a bulletproof, end-to-end blueprint. This is the final, shippable plan. 💀

0) One-liner goal

Auto-discover models/datasets/prompts/tools across repos → emit standards-compliant CycloneDX ML-BOM → store/version in TiDB (VECTOR + FULLTEXT or fallback) → diff → policy check → notify (Slack/Jira) — with full provenance & audit.


---

1) Deliverables (meets hackathon rules)

Working app (API + minimal UI) with multi-step agent using TiDB Serverless + vector + (full-text or fallback) + external APIs.

Repo (MIT/Apache-2.0 for Open-Source award), README with one-command run, data-flow summary, 3-min demo video.

Live demo or test creds, seeds to reproduce.



---

2) Architecture (cross-compatible)

Services

1. ScanGit: walk repo(s), collect candidates (py/ipynb/yaml/json/md/prompts).


2. ScanHF: fetch model/dataset cards & licenses (cache).


3. Normalize: classify artifacts → model/dataset/prompt/tool (+ hashes, SHAs, paths).


4. Embed+Index: chunk evidence → embed (1536-dim) → TiDB VECTOR.


5. BOMGen: output CycloneDX ML-BOM JSON (validates against schema).


6. Diff: compare latest vs previous BOM → structured diff summary.


7. Policy: evaluate rules (license, drift, unknowns, protected paths).


8. Notify: MCP tools → Slack/Jira (action log in DB).


9. UI: one page — Run Scan, show BOM, Diff, Policy events, Actions.



Fallbacks

If FULLTEXT unsupported in your TiDB region, fallback to:
a) BM25 in app-layer (e.g., rank_bm25 via Python), or
b) LIKE/token tables. Hybrid still works: ANN (DB) + BM25 (app), fuse with RRF.



---

3) TiDB schema (hardened + provenance)

CREATE TABLE projects(
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  name VARCHAR(255) UNIQUE,
  repo_url TEXT,
  default_branch VARCHAR(64)
);

CREATE TABLE models(
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  project_id BIGINT, name VARCHAR(255), provider VARCHAR(128),
  version VARCHAR(128), license VARCHAR(128),
  source_url TEXT, repo_path TEXT, commit_sha CHAR(40), meta JSON,
  CONSTRAINT fk_models_project FOREIGN KEY (project_id) REFERENCES projects(id),
  UNIQUE KEY uk_model (project_id, name, provider, version)
);

CREATE TABLE datasets(
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  project_id BIGINT, name VARCHAR(255), version VARCHAR(128),
  license VARCHAR(128), source_url TEXT, commit_sha CHAR(40), meta JSON,
  CONSTRAINT fk_datasets_project FOREIGN KEY (project_id) REFERENCES projects(id),
  UNIQUE KEY uk_dataset (project_id, name, version)
);

CREATE TABLE prompt_blobs(                -- dedupe large prompt texts
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  sha CHAR(64) UNIQUE,
  content LONGTEXT
);

CREATE TABLE prompts(
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  project_id BIGINT, name VARCHAR(255), version VARCHAR(64),
  path TEXT, commit_sha CHAR(40), blob_sha CHAR(64), meta JSON,
  CONSTRAINT fk_prompts_project FOREIGN KEY (project_id) REFERENCES projects(id),
  KEY idx_prompts_blob (blob_sha)
);

CREATE TABLE tools(
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  project_id BIGINT, name VARCHAR(255), version VARCHAR(128),
  type ENUM('api','lib','mcp'), spec JSON, meta JSON,
  CONSTRAINT fk_tools_project FOREIGN KEY (project_id) REFERENCES projects(id)
);

CREATE TABLE evidence_chunks(
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  project_id BIGINT, ref_type ENUM('file','card','config','readme'),
  ref_path TEXT, commit_sha CHAR(40), chunk_ix INT,
  text LONGTEXT, token_count INT,
  emb VECTOR(1536) /* CHECK dim=1536 if supported */,
  meta JSON,
  FULLTEXT KEY ft_text (text),               -- ok if FTS available; else ignore
  KEY idx_evidence_proj_type (project_id, ref_type),
  CONSTRAINT fk_evidence_project FOREIGN KEY (project_id) REFERENCES projects(id)
);

CREATE TABLE boms(
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  project_id BIGINT, bom_json JSON,
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  CONSTRAINT fk_boms_project FOREIGN KEY (project_id) REFERENCES projects(id)
);

CREATE TABLE bom_diffs(
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  project_id BIGINT, from_bom BIGINT, to_bom BIGINT,
  summary JSON, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  CONSTRAINT fk_diffs_from FOREIGN KEY (from_bom) REFERENCES boms(id),
  CONSTRAINT fk_diffs_to   FOREIGN KEY (to_bom)   REFERENCES boms(id)
);

CREATE TABLE policies(
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  rule VARCHAR(255), severity ENUM('low','medium','high'),
  spec JSON -- e.g., allowlist licenses, protected paths, version bump thresholds
);

CREATE TABLE policy_overrides(
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  project_id BIGINT, rule VARCHAR(255),
  until DATETIME, reason TEXT
);

CREATE TABLE policy_events(
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  project_id BIGINT, severity ENUM('low','medium','high'),
  rule VARCHAR(255), artifact JSON, details JSON,
  dedupe_key VARCHAR(255),        -- collapse repeats within 24h
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE suppressions(
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  event_id BIGINT, reason TEXT
);

CREATE TABLE actions(
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  project_id BIGINT, kind ENUM('slack','jira','email'),
  payload JSON, response JSON, status ENUM('ok','fail') DEFAULT 'ok',
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);


---

4) Retrieval: hybrid (ANN + BM25) with RRF

Step A (ANN in TiDB):

SELECT id, VEC_COSINE_DISTANCE(emb, :qvec) AS d
FROM evidence_chunks
WHERE project_id=:pid
ORDER BY d ASC
LIMIT 50;

Step B (BM25):

If TiDB FULLTEXT exists:


SELECT id, MATCH(text) AGAINST (:q IN NATURAL LANGUAGE MODE) AS bm25
FROM evidence_chunks
WHERE project_id=:pid
ORDER BY bm25 DESC
LIMIT 50;

Else: compute BM25 in app on fetched candidates (keep K small).


Step C (RRF fuse): rank both → score = 1/(60+rank_ann) + 1/(60+rank_bm25) → top-K.

Pre-norm: lowercase, strip code fences, token-chunk (by ~700–900 tokens), store token_count.


---

5) Orchestration (LangGraph DAG)

1. ScanPlan(input: project) → targets (repo paths, HF slugs).


2. ScanGit → file list, commit_sha, paths; extract candidates.


3. ScanHF → model/dataset cards, licenses, versions (cache by slug+ver).


4. Normalize → artifact classification + hashes (sha256) + SPDX license map (use scancode/licensee subprocess; unknown => high).


5. Embed+Index → batch embed new chunks; upsert with hashes; rate limit.


6. BOMGen → CycloneDX ML-BOM; validate JSON schema; insert boms.


7. DiffPrev → compare stable IDs; insert bom_diffs.


8. PolicyCheck → evaluate rules + overrides; insert policy_events (with dedupe_key).


9. Notify → MCP Slack/Jira; insert actions with request/response snippets.


10. End → return BOM id, diff summary, events, action ids.



Guards

Tool allowlist (Slack, Jira, Email only).

JSON Schemas for node I/O.

Timeouts per node; retries (exponential backoff).



---

6) Policies (starter set)

missing_license (high): artifact lacks SPDX-mapped license.

unknown_provider (med): model/dataset source URL unknown.

model_bump_major/minor (med/low): semantic bump rules.

prompt_changed_protected_path (high): prompt hash change under /prompts/ or /prod.

unapproved_license (high): not in allowlist.

dataset_origin_changed (med): source_url changed.


Overrides & noise control

policy_overrides to waive until date.

Collapse repeats via dedupe_key within 24h.



---

7) UI (minimal but judge-friendly)

Project selector + Run Scan.

Tabs: BOM, Diff, Policy, Actions.

Buttons: Open Jira ticket, Send Slack summary, Dry-run (no DB writes).

Replay: load previous scan by boms.id.



---

8) Security & audit

Secrets in env/VAULT; never store tokens in DB.

Every outbound call logged in actions with minimal request/response snippets.

Dry-run mode for safe demos.

MCP tool allowlist & domain restrictions.



---

9) Idempotency & scale

Hash-based upserts for prompts/evidence; skip unchanged.

Batch inserts; paginate embeddings.

Cache HF cards by slug+version with TTL.

Queue scans per project (one active at a time).



---

10) Testing traps (and fixes)

Notebooks: parse code cells only; ignore outputs.

YAML: scan keys (model, checkpoint, dataset, prompt, tokenizer).

Prompts: glob /prompts/**, *.prompt, .md sections; normalize whitespace before hashing.

BOM schema: validate pre-insert; reject if invalid.

Diff stability: stable component IDs; ignore non-semantic fields (timestamps, formatting).



---

11) Feasibility (per step) — honest

Step	You	Difficulty	Est. hrs	Notes

TiDB setup & DDL	OK	Low	6–8	Stick to MySQL features; FTS fallback ready.
ScanGit	OK	Med	10–14	Use gitpython or shell; handle ignore rules.
ScanHF cache	OK	Low	4–6	Simple GET + local/DB cache.
Normalize + SPDX	New	Med	8–10	Add scancode/licensee subprocess; map to SPDX.
Embeddings + VECTOR	OK	Med	6–10	Batch+hash; 1536-dim enforced.
FULLTEXT/BM25	New	Med	6–8	Use TiDB FTS if available; otherwise app BM25.
CycloneDX ML-BOM	New	Med	6–8	Use pydantic models / JSON schema.
Diff engine	OK	Med	6–8	Structural compare with stable IDs.
Policy engine	OK	Med	6–8	6–8 rules + overrides + dedupe.
MCP Slack/Jira	OK	Med	6–10	Service accounts; JSON payload tests.
UI minimal	OK	Low	6–8	Streamlit or FastAPI+HTMX.
Docs + video	OK	Low	6–8	Write as you build.


Total: ~72–104 hrs → 10–14 days @ 6–8 hrs/day, or 18–21 days @ 4–5 hrs/day. Solo doable.


---

12) Demo script (3 minutes)

1. Select project → Run Scan → BOM v1 stored; show TiDB tables updating.


2. Change model: llama-3.1-8b → 70b and tweak dataset license in sample repo → Run Scan.


3. Show Diff (major model bump + license change).


4. Policy events fire → click Open Jira + Send Slack → show actions rows.


5. Quick peek at evidence list supporting each finding.




---

13) Repo skeleton

ai-bom-autopilot/
  apps/
    api/           # FastAPI endpoints
    ui/            # Streamlit or FastAPI+HTMX
  core/
    scan_git/      # repo walker + heuristics
    scan_hf/       # HF card fetch + cache
    normalize/     # classifiers, SPDX map, scancode/licensee adapter
    embeddings/    # batcher, hashing
    bom/           # CycloneDX builders + schema validate
    diff/          # structural diff
    policy/        # rule engine + overrides + dedupe
    db/            # TiDB client, queries, migrations
    mcp_tools/     # slack/, jira/, email/
    schemas/       # pydantic models
    graph/         # LangGraph DAG
  infra/
    docker-compose.yml
    migrations/
    seed/          # sample monorepo, HF slugs, fake prompts
  tests/
    unit/, e2e/


---

14) Acceptance criteria (win checklist)

[ ] End-to-end run completes on seed project with BOM v1 → v2.

[ ] CycloneDX ML-BOM validates (schema).

[ ] Hybrid retrieval returns evidence list for flagged items.

[ ] At least 2 external tools used (Slack + Jira).

[ ] Policy overrides work; duplicate events deduped.

[ ] README: one-command run; .env.example; architecture diagram.

[ ] <3-min demo video shows scan, diff, policy, notifications.



---

15) What changed vs previous blueprint (your patch list applied)

Added projects table; repo_path/commit_sha on all entities; env/container digest optional in meta.

Composite UNIQUEs; project_id FKs everywhere.

prompt_blobs table (content de-dup by sha).

evidence_chunks got token_count, chunk_ix, commit_sha, indexes.

Fixed VECTOR dim (1536) and token-based chunking.

RRF specified; both scores logged.

Policy tables: policies, policy_overrides, suppressions; realistic rules added.

License detection via scanner + SPDX map; unknown = high severity.

Idempotent upserts, batching, rate limits, queue scans.

Secrets hygiene & full actions audit.

Dry-run, warm cache, noise control.



---

Final verdict

This plan is feasible solo, meets every hackathon criterion, is differentiated, and is resume-ready for a 5th-sem DSE with 6.3 CGPA. If you want, I’ll generate the starter repo (folders + migrations + LangGraph skeleton + seed project) next so you can start tonight. 🔥
[11/08/25, 2:33:50 PM] ∆: Master — short, brutal truth:

Is it scalable?

Yes, with sane architecture.

Stateless services + queue: scanner/normalizer/embeddings as workers; API/UI stateless; TiDB as source-of-truth.

TiDB path: start Serverless (auto-scale); for >~10M chunks move to TiDB Dedicated (bigger caches, HTAP).

Storage math (embeddings 1536-dim float32):

Per chunk ≈ 6 KB for vector.

1M chunks ≈ 6 GB (embeddings only); 10M ≈ 60 GB. Text adds ~1–3 GB per 1M chunks depending on avg chunk size.


Throughput: 8 workers × 15 embeds/sec each ≈ 120 chunks/sec → ~432k/day (24h). Scale workers horizontally.

Cost control: hash-based idempotency, batch embeddings, cache HF cards, nightly TTL for stale evidence.


Is it monetizable?

Yes, compliance budgets are real.

Buyers: Head of AI/ML, Sec/Compliance, Data Gov in finance, health, pharma, SaaS.

Value: audit readiness (EU AI Act/GPAI), vendor/risk transparency, faster due-diligence.

Pricing (viable ranges):

SaaS Team: $499–$1,499/month (limits on projects/chunks).

Enterprise: $25k–$150k/year (SSO, on-prem, SLAs).

Pro Services: $5k–$20k for one-off audit setup.


GTM wedge: free CLI/GitHub Action that emits ML-BOM locally → upsell to hosted policy, diff, alerts.


Moat (why they’ll pay you, not DIY)

Standards-native (CycloneDX ML-BOM + SPDX),

Policy packs (license allowlists, governance mappings),

Attestations & audit trail (Jira/Slack tickets, signed diffs),

CI gates (block merges on violations).
These aren’t trivial scripts; they’re process & proof.


Risks (and answers)

“We can script this.” → They won’t maintain policy packs/attestations; you sell governance + auditability.

DB bloat. → Compress/TTL old evidence; store blobs in object storage; keep only hashes + vectors in DB.

LLM/emb cost. → Use open-source emb (e5-small) on CPU; only use paid LLM for edge classification.


Deployment options (to close deals)

Cloud SaaS (multi-tenant) for SMBs.

BYO TiDB Cloud (customer’s account) for regulated customers.

On-prem (Docker/K8s) with offline modes (no code leaves VPC).


Does it meet hackathon requirements?

Yes. Multi-step agent + TiDB (vector + full-text/fallback) + external APIs (Slack/Jira) + clear demo and docs. Novel vs typical RAG toys. 🔥

Bottom line

Scalable? Yes — horizontally with predictable storage/throughput.

Monetizable? Yes — compliance budgets + enterprise need now.

Resume? Killer: compliance-grade AI governance tool, not another chatbot.


Want me to spin a starter repo with worker queue, TiDB migrations, seed project, and policy pack v1 so you can push a live demo this week?
[11/08/25, 2:36:44 PM] ∆: Yes, Master—100% doable with just your MacBook Air M2 + Wi-Fi + Kimi (or Kiro) + me. No GPU needed.

What you’ll run locally

Python 3.11+ (uv or pipenv), FastAPI (API), Streamlit/HTMX (minimal UI)

LangGraph (agent flow), TiDB client (MySQL driver)

BM25 (app-layer) lib if TiDB FULLTEXT isn’t available in your region

License scanner (scancode/licensee via subprocess) — CPU ok

Embeddings via Kimi/OpenRouter or OpenAI—API call = no heavy compute


What runs in the cloud (no hardware from you)

TiDB Serverless (DB + VECTOR; FULLTEXT if supported in your region)

Hugging Face (pull model/dataset cards over HTTP)

Slack/Jira APIs (notifications)


Performance on M2 (realistic)

Embedding throughput (API): ~50–150 chunks/min (depends on provider limits)

Scan + normalize a medium repo (2–5k files): 3–10 min

End-to-end scan→BOM→diff→policy→notify on seed project: <3 min (demo path)


Possible bottlenecks (and fixes)

TiDB FULLTEXT not available → use BM25 in app-layer; still hybrid with vectors.

Rate limits (Kimi/OpenRouter/HF) → batch + cache by hash; queue scans (1 per project at a time).

License scanner slow on huge repos → only scan files flagged by the normalizer (py/ipynb/yaml/md/prompts).


Minimal install on your Mac

# env
brew install uv mysql-client
uv venv
uv pip install fastapi uvicorn[standard] langgraph pydantic streamlit gitpython python-decouple
uv pip install scikit-learn rank-bm25 # if doing app-layer BM25
# optional license tools (choose one)
brew install scancode-toolkit || brew install licensee

Config you need

TIDB_URL, DB_USER, DB_PASS

LLM/Embedding key (Kimi/OpenRouter/OpenAI)

SLACK_WEBHOOK or Bot token, JIRA_URL + API token


Verdict

Feasible on your M2 Air: yes.

Network-only dependencies: TiDB Cloud + LLM/embedding API + HF + Slack/Jira.

No GPU / no Docker mandatory (Docker optional for clean demo).

You can ship the MVP in ~18–21 days solo at your normal pace.


If you want, I’ll generate the starter repo now (folders, migrations, LangGraph DAG skeleton, seed project, .env.example) so you can just plug keys and run. 🔥
[11/08/25, 2:37:20 PM] ∆: Here’s your short, no-bloat hackathon checklist, Master 💀🔥


---

Hackathon Checklist – TiDB AgentX 2025

Theme Fit:

[ ] Multi-step agent (not single-shot)

[ ] Uses TiDB Serverless + vector search

[ ] Uses full-text search (or fallback BM25 in app-layer)

[ ] Integrates ≥ 2 external APIs/tools (e.g., Slack, Jira, HF)


Submission Package:

[ ] Public repo URL (OSI license if aiming for Open Source award)

[ ] README:

Install/run instructions (one command)

Data-flow summary diagram

Env variables list (.env.example)


[ ] Working demo (link or test creds)

[ ] <3-min demo video (clear, shows workflow end-to-end)

[ ] Text description of features & function


Judging Criteria (100 pts):

[ ] Tech Implementation – 35 pts (solid TiDB use, clean architecture)

[ ] Creativity/Idea – 25 pts (novel, real problem)

[ ] User Experience – 20 pts (simple, clear UI)

[ ] Documentation – 10 pts (complete, readable)

[ ] Demo Video – 10 pts (fast, compelling)


Deadline:

[ ] Submit by Sep 15, 2025, 11:45 PM PDT

[ ] Register before submission cutoff



---

If you want, I can also make you a day-by-day execution checklist so you stay on track and don’t miss any of these boxes.








Master — short, surgical take 💀

Verdict: Greenlight. It’s a strong, shippable hackathon play with real enterprise angles. No fatal flaws. Worth building.

Major risks (fix before demo):
	•	Spec risk: ML-BOM schema/version churn. Lock to a specific CycloneDX ML-BOM draft/version, bundle a local JSON Schema, and validate against that in CI. Don’t chase “latest” during the hack.
	•	Licenses are messy: HF cards often have non-SPDX or missing licenses. Define a strict SPDX map + “unknown ⇒ high severity,” and make overrides obvious in UI to avoid noise.
	•	Recall gaps: Prompt/model/dataset discovery can miss stuff embedded in code or env vars. Add lightweight heuristics: scan for common provider SDK calls + known file patterns + .env keys; surface “suspected but unconfirmed” with evidence.
	•	Policy noise → demo fail: Default rules can fire too much. Ship a Policy Pack v1 (low-noise) + one-click “demo mode” seeds so judges see crisp signals.
	•	Vector/FTS variability: TiDB FULLTEXT/vector index configs differ by region. On startup, self-test capabilities; if FTS missing, auto-enable app-BM25 path and log it in UI (“Hybrid: ANN+BM25”).

Scalability reality check:
	•	Storage math and worker model are fine. For 10M+ chunks: push blob text to object storage; keep only sha + vector + small text preview in TiDB. Ensure vector index (HNSW/IVF) parameters are tunable per project.
	•	Queue scans per project (already planned) + backpressure on embeddings. Add dead-letter for bad repos.

Monetization (near-term):
	•	Good wedge: free GitHub Action → ML-BOM.json in repo → hosted diff/policy/alerts. Compliance teams will take the meeting. Price bands you listed are realistic.

MVP trims (to hit demo cleanly):
	•	Slack only (Jira optional).
	•	One embedding provider.
	•	Limit artifacts: Git repos + HF model cards (datasets optional).
	•	5 policies max (missing_license, unapproved_license, major_model_bump, protected_prompt_change, unknown_provider).
	•	Streamlit UI with 3 tabs and a single “Run Scan”.

Add these tiny but critical upgrades:
	•	Canonical IDs: project:name:kind:provider:version so diffs are stable.
	•	Provenance clickthrough: every policy finding links to the exact evidence chunk + commit SHA.
	•	Health checks: startup page shows DB/vector/FTS/keys status 🟢🟡🔴.
	•	Demo seed: scripted repo change (model 8B→70B + license tweak) to guarantee a spicy diff.
	•	Audit hash: sign BOM & diff (SHA256) and display the hash in UI (“copy to clipboard”).

Competitor reality: There are governance/compliance tools circling this space, but few ship standards-native ML-BOM + diff + policy + evidence end-to-end. Your angle is solid if you nail attestations + CI gates.

Bottom line: Yes, it’s a good idea and worth it. Ship the lean path above, avoid license/policy noise, and you’ll demo like a sniper. Want me to spit out the starter repo skeleton + migrations + policy pack v1 now? 🔥








combine all these messages into one ready to build doc 
do all the searching , combatibilty , feasibilty etc 


Master — GO. It’s tight enough to start building today. 🔥

Why I’m green-lighting it
	•	Clear goal + scope, no bloat.  ￼
	•	Hits hackathon requirements (agentic app, TiDB vector/FTS or fallback, ext. APIs, public repo + demo).  ￼  ￼
	•	Architecture + schema are concrete and cross-compatible.  ￼  ￼
	•	FTS variability handled (auto-fallback to BM25).  ￼
	•	Feasible solo in ~10–14 days focused.  ￼
	•	Win checklist + demo script already defined.  ￼  ￼

Pre-flight fixes (do these now, then code)
	1.	Pin CycloneDX ML-BOM v1.6 and validate locally/CI.  ￼
	2.	Self-test TiDB features at startup → if FULLTEXT fails, switch to app-BM25; keep hybrid via RRF.  ￼  ￼
	3.	Enforce 1536-dim embeddings + HNSW index; chunk by tokens.  ￼
	4.	Ship low-noise Policy Pack v1 + overrides to avoid alert spam.  ￼  ￼
	5.	Secrets hygiene + action audit (log outbound Slack/Jira).  ￼
	6.	Scripted demo seed (8B→70B + license tweak) and follow the 3-min flow.  ￼
	7.	Submission checklist (repo, one-command run, .env.example, short video).  ￼  ￼

Known risks (already accounted for)
	•	Spec churn + messy licenses → pin schema + strict SPDX map, “unknown ⇒ high”.  ￼
	•	Discovery recall gaps → heuristics for SDK calls/.env/patterns.  ￼

Bottom line: You’re cleared to build. Start with migrations + startup self-test, then the scan → BOM → diff → policy → notify golden path. If you want, I’ll drop a starter repo next so you just plug keys and run. 💀🗣️


Alright Master — here’s the updated full blueprint for the Runtime AI-BOM via eBPF concept.
This keeps the TiDB-heavy architecture from your original, but pivots to runtime telemetry — making it far rarer and more defensible as “novel.”

⸻

Runtime AI-BOM Autopilot

0) One-Liner Goal

Live-capture all ML/AI components actually used at runtime (models, datasets, prompts, tools) via eBPF syscall tracing → generate CycloneDX ML-BOM → store in TiDB → diff → policy → alert.

⸻

1) Why It’s Novel
	•	Vendors generate AI-BOM from static repos — few capture runtime usage.
	•	eBPF lets you hook into kernel-level events (file opens, network pulls) with negligible overhead.
	•	Ties CycloneDX ML-BOM standard to runtime telemetry for the first time (in a hackathon setting).
	•	Perfect TiDB sponsor fit: vector search on evidence chunks + full-text fallback, storing BOM history with provenance.

⸻

2) Deliverables
	•	Agent: eBPF-based tracer (bcc/libbpf/py-bpfcc) that logs model/dataset/prompt access in real time.
	•	BOM Generator: Normalize runtime events to CycloneDX ML-BOM 1.6 JSON.
	•	DB layer: TiDB schema for projects, runtime artifacts, embeddings, BOMs, diffs, policies, evidence.
	•	Policy engine: Rules for unapproved model loads, license issues, prompt changes.
	•	UI: Run scan, view BOM, diffs, triggered policies, evidence click-through.
	•	Demo: Launch sample ML app → see BOM auto-populate → change model → see diff + alerts.

⸻

3) Architecture

Flow:
	1.	eBPF Tracer: Hooks open(), read(), connect() syscalls. Filters events for:
	•	File extensions: .pt, .bin, .safetensors, .json, .yaml
	•	Paths containing /models/, /datasets/, /prompts/
	•	Network calls to HF/CDN endpoints
	2.	Event Normalizer: Maps to {type: model/dataset/prompt, name, path, source_url, commit_sha?}; queries HF API for license/meta if applicable.
	3.	BOM Generator: CycloneDX ML-BOM (component type, version, hashes, licenses).
	4.	Embed & Index: Text chunks from metadata → 1536-dim embeddings → store in TiDB VECTOR(1536) with HNSW.
	5.	BOM Diff: Compare to last BOM; stable IDs ensure accurate change tracking.
	6.	Policy Engine: License allowlist, unapproved provider, version drift, protected path change.
	7.	Notify: Slack/Jira integration.
	8.	UI: Single-page: BOM tab, Diff tab, Policy tab, Actions tab.

⸻

4) TiDB Schema (Additions for Runtime)

CREATE TABLE runtime_events (
  id BIGINT PRIMARY KEY AUTO_INCREMENT,
  project_id BIGINT NOT NULL,
  ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
  pid INT,
  process_name VARCHAR(255),
  syscall VARCHAR(64),
  path TEXT,
  source_url TEXT,
  type ENUM('model','dataset','prompt','tool') NOT NULL,
  hash CHAR(64),
  meta JSON,
  emb VECTOR(1536),
  KEY idx_proj_ts (project_id, ts),
  FULLTEXT KEY ft_path (path) -- if supported
);

-- BOMs, bom_diffs, policies, policy_events remain as in your original plan


⸻

5) Retrieval
	•	ANN search: Find similar runtime artifacts across history.
	•	FTS/BM25 fallback: Search by path/name/license.
	•	RRF fusion: Merge ANN + keyword results.

⸻

6) Policy Pack v1
	•	missing_license (high)
	•	unapproved_provider (high)
	•	model_bump_major/minor (med/low)
	•	protected_prompt_change (high)
	•	unknown_dataset_origin (med)

⸻

7) Demo Plan (3 minutes)
	1.	Start mlbom-agent (eBPF tracer).
	2.	Run sample ML app: loads bert-base-uncased from HF → runtime event captured → BOM v1 in UI.
	3.	Swap model to bert-large + dataset with unknown license → rerun.
	4.	BOM v2 appears → Diff shows model bump + license issue → policy flags → Slack/Jira alert.
	5.	Click policy → evidence shows file path, PID, and syscall trace.

⸻

8) Tech Stack
	•	Python (FastAPI backend, Streamlit/HTMX UI)
	•	eBPF (bcc/libbpf, py-bpfcc)
	•	TiDB Serverless (VECTOR, FTS)
	•	OpenAI/HF embeddings
	•	CycloneDX Python Lib
	•	Slack/Jira Webhooks

⸻

9) Build Feasibility
	•	eBPF file access hook + JSON log: 8–12h
	•	Event normalizer + HF API meta fetch: 4–6h
	•	BOM gen + TiDB ingest: 8–10h
	•	Embedding + HNSW index: 6–8h
	•	Policy engine: 6–8h
	•	UI: 8–10h
	•	Demo repo + docs: 6–8h
Total: ~46–62h (solo), safe for hackathon.

⸻

10) Risks & Mitigation
	•	eBPF permissions: Run in dev container with --privileged for demo.
	•	Noise filtering: Path/extension allowlist to reduce irrelevant events.
	•	FTS support: Detect at startup, fallback to BM25 if missing.
	•	Licenses: Cache HF license lookups.

⸻

If you want, Master, I can now rewrite your Devpost “About the project” and elevator pitch to reflect this runtime eBPF pivot so judges see the novelty right away.
Do you want me to prepare that next?